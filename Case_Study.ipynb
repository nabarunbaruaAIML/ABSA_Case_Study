{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ABSA Case Study\n",
        "You are a data scientist who has been tasked with developing a versatile ABSA model for extracting aspects, determining aspect polarity, and detecting aspect categories from textual data. The goal is to create a robust model/pipeline ensuring flexibility and accuracy across different contexts.\n",
        "\n",
        "Aspect-Based Sentiment Analysis (ABSA) is a natural language processing (NLP) technique that involves extracting and analysing sentiment or emotion associated with specific aspects or features of a given target entity, such as a product, service, or topic.  \n",
        "\n",
        "Data Description:\n",
        "Your toolkit will include dataset comprising of 3000 customer reviews for a restaurant, all in English, enriched with human-authored annotations. These annotations contain the mentioned aspects of the target entities and the sentiment polarity of each aspect.\n",
        "\n",
        "\n",
        "Tasks:\n",
        "\n",
        "•\tABSA Model Development: Create an ABSA model that can extract aspect terms, determine aspect polarity, and identify aspect categories within a given text. Ensure that the model is generic and can be applied to different domains with ease.\n",
        "•\tResults Analysis: Analyze the results generated by the ABSA model. Use matplotlib/ plotly to show the overall as well category wise sentiment distribution. \n",
        "•\tActionable insights: Based on the above analysis, derive the conclusions and identify action items for the restaurant to work-upon.\n",
        "•\tReport: Discuss pros and cons of the approach you selected and further improvement which can be done to it given more time.\n",
        "\n",
        "\n",
        "In particular, the task 1 consists of the following subtasks: \n",
        " \n",
        "Definition 1: Aspect term extraction \n",
        "For the given entity -  restaurant, identify the aspect terms present in the sentence and return a list containing all the distinct aspect terms. An aspect term names a particular aspect of the target entity(Restaurant). \n",
        "  \n",
        "For example, \"I liked the service and the staff, but not the food”, “The food was nothing much, but I loved the staff”. Here service, staff and food are aspects.\n",
        "\n",
        " Note: Multi-word aspect terms (e.g., “hard disk”) should be treated as single terms (e.g., in “The hard disk is very noisy” the only aspect term is “hard disk”). \n",
        " \n",
        "Definition 2: Aspect term polarity \n",
        "For a given set of aspect terms within a sentence, determine whether the polarity of each aspect term is positive, negative, neutral or conflict (i.e., both positive and negative). \n",
        "  \n",
        "For example: \n",
        " \n",
        "“I loved their fajitas” → {fajitas: positive} \n",
        "“I hated their fajitas, but their salads were great” → {fajitas: negative, salads: positive} \n",
        "“The fajitas are their first plate” → {fajitas: neutral} \n",
        "“The fajitas were great to taste, but not to see” → {fajitas: conflict} \n",
        " \n",
        "Definition 3: Aspect category detection \n",
        "Decide a predefined set of aspect categories (e.g., price, food, service etc.) for restaurant, identify the aspect categories discussed in a given sentence. Aspect categories are typically coarser than the aspect terms of Subtask 1, and they do not necessarily occur as terms in the given sentence. \n",
        "  \n",
        "For example, given the set of aspect categories {food, service, price, ambience, anecdotes/miscellaneous}: \n",
        "“The restaurant was too expensive”  → {aspect -expensive, category - price} \n",
        "“The restaurant was expensive, but the menu was great” → { aspect -expensive, category - price, aspect- menu, category - food} \n",
        "\n",
        "Definition 4:\n",
        "Action Items: Areas which can be improved to improve the overall sentiment of the customer.\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### List Packages we need to Install\n",
        "- xmltodict\n",
        "- pandas\n",
        "- nltk\n",
        "- matplotlib\n",
        "- plotly\n",
        "- torch\n",
        "- transformers\n",
        "- datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhLQ22mYSXZM",
        "outputId": "246a191b-0ccc-4e41-b806-dc5ad887f33f"
      },
      "outputs": [],
      "source": [
        "# !pip install xmltodict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the XML Data and Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwuEKnDFRj-P",
        "outputId": "374cacaf-edb7-414a-e792-c0cdf9aa4bab"
      },
      "outputs": [],
      "source": [
        "import xmltodict\n",
        "\n",
        "# Open and read the XML file\n",
        "with open('Restaurants_Train_v2.xml', 'r') as f:\n",
        "    xml_content = f.read()\n",
        "\n",
        "# Convert the XML to a dictionary\n",
        "xml_dict = xmltodict.parse(xml_content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Let's First see how that Data Looks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'@id': '3121', 'text': 'But the staff was so horrible to us.', 'aspectTerms': {'aspectTerm': {'@term': 'staff', '@polarity': 'negative', '@from': '8', '@to': '13'}}, 'aspectCategories': {'aspectCategory': {'@category': 'service', '@polarity': 'negative'}}}, {'@id': '2777', 'text': \"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\", 'aspectTerms': {'aspectTerm': {'@term': 'food', '@polarity': 'positive', '@from': '57', '@to': '61'}}, 'aspectCategories': {'aspectCategory': [{'@category': 'food', '@polarity': 'positive'}, {'@category': 'anecdotes/miscellaneous', '@polarity': 'negative'}]}}, {'@id': '1634', 'text': \"The food is uniformly exceptional, with a very capable kitchen which will proudly whip up whatever you feel like eating, whether it's on the menu or not.\", 'aspectTerms': {'aspectTerm': [{'@term': 'food', '@polarity': 'positive', '@from': '4', '@to': '8'}, {'@term': 'kitchen', '@polarity': 'positive', '@from': '55', '@to': '62'}, {'@term': 'menu', '@polarity': 'neutral', '@from': '141', '@to': '145'}]}, 'aspectCategories': {'aspectCategory': {'@category': 'food', '@polarity': 'positive'}}}, {'@id': '2534', 'text': 'Where Gabriela personaly greets you and recommends you what to eat.', 'aspectCategories': {'aspectCategory': {'@category': 'service', '@polarity': 'positive'}}}, {'@id': '583', 'text': \"For those that go once and don't enjoy it, all I can say is that they just don't get it.\", 'aspectCategories': {'aspectCategory': {'@category': 'anecdotes/miscellaneous', '@polarity': 'positive'}}}, {'@id': '2846', 'text': \"Not only was the food outstanding, but the little 'perks' were great.\", 'aspectTerms': {'aspectTerm': [{'@term': 'food', '@polarity': 'positive', '@from': '17', '@to': '21'}, {'@term': 'perks', '@polarity': 'positive', '@from': '51', '@to': '56'}]}, 'aspectCategories': {'aspectCategory': [{'@category': 'food', '@polarity': 'positive'}, {'@category': 'service', '@polarity': 'positive'}]}}, {'@id': '1571', 'text': 'It is very overpriced and not very tasty.', 'aspectCategories': {'aspectCategory': [{'@category': 'food', '@polarity': 'negative'}, {'@category': 'price', '@polarity': 'negative'}]}}, {'@id': '1458', 'text': 'Our agreed favorite is the orrechiete with sausage and chicken (usually the waiters are kind enough to split the dish in half so you get to sample both meats).', 'aspectTerms': {'aspectTerm': [{'@term': 'orrechiete with sausage and chicken', '@polarity': 'positive', '@from': '27', '@to': '62'}, {'@term': 'waiters', '@polarity': 'positive', '@from': '76', '@to': '83'}, {'@term': 'meats', '@polarity': 'neutral', '@from': '152', '@to': '157'}, {'@term': 'dish', '@polarity': 'neutral', '@from': '113', '@to': '117'}]}, 'aspectCategories': {'aspectCategory': [{'@category': 'food', '@polarity': 'positive'}, {'@category': 'service', '@polarity': 'positive'}]}}, {'@id': '3161', 'text': 'The Bagels have an outstanding taste with a terrific texture, both chewy yet not gummy.', 'aspectTerms': {'aspectTerm': {'@term': 'Bagels', '@polarity': 'positive', '@from': '4', '@to': '10'}}, 'aspectCategories': {'aspectCategory': {'@category': 'food', '@polarity': 'positive'}}}, {'@id': '2391', 'text': 'Nevertheless the food itself is pretty good.', 'aspectTerms': {'aspectTerm': {'@term': 'food', '@polarity': 'positive', '@from': '17', '@to': '21'}}, 'aspectCategories': {'aspectCategory': {'@category': 'food', '@polarity': 'positive'}}}]\n"
          ]
        }
      ],
      "source": [
        "print(xml_dict['sentences']['sentence'][0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3041\n"
          ]
        }
      ],
      "source": [
        "print(len(xml_dict['sentences']['sentence']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First Observation what we can understand from the Data\n",
        "- It has total 3041 Text and ID's\n",
        "- Aspect Term: Some place it has multiple Data, some places only single or No Entries.\n",
        "    - Each Term is coming with label Positive or Negative or Neutral\n",
        "- Aspect Category: This also in some place Single Data or Multiple Data\n",
        "    - Similar to Term, it also have each category has label Positive or Negative or Neutral(Not shown here but assuming that is the case)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First Impression and Understanding\n",
        "\n",
        "After reading and seeing the dataset, the first impression for the task ABSA Model Development are below:\n",
        "    - For TERM's the basic impression is NER that would extract words which is Important to Resttaurant Business. \n",
        "    - For TERM we also need to Sentence Inferencing between extracted NER and Text to find Polarity i.e. Positive, Negative & Neutral. \n",
        "    - For Category we need to do Multi-Label sentence Classification for given labels: food, service, price, ambience, anecdotes/miscellaneous.\n",
        "    - For Category also we have to do Sentence Inferencing between Category and Text to find Polarity i.e. Positive, Negative & Neutral. Since in Sample Data viewing we can't find Neutral therefore this is still assumption that there are three labels, will Do EDA to find it\n",
        "\n",
        "**With above thought we have to do NER and Multi-Label Sentence Classification training at least and for Sentence Inferencing we will see if we need to train Sentence Inferencing**\n",
        "\n",
        "\n",
        "**I will try to find if any NER and Multi-Label Sentence Classification models in huggingface hub is available or not which will act as a baseline**  \n",
        "\n",
        "\n",
        "**Data will have to be splitted between Train and Test as there is not seperate test data was shared with the package**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## We can't use XML Data as given, so we are doing some pre-processing. We would convert data for NER, Multi-Label Sentence Classification and Sentence Inferencing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **get_clean_text:** This function is used for Text clean-up. Found some corner case in the text which causing exception so doing pre-prcessing on the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def get_clean_text(text):\n",
        "    # Step 1: Remove single or double quotes surrounding words\n",
        "    cleaned_string = re.sub(r\"(['\\\"])(\\w+)\\1\", r\"\\2\", text)\n",
        "\n",
        "    # Step 2: Remove any remaining special characters except apostrophes\n",
        "    cleaned_string = re.sub(r\"[^A-Za-z0-9\\s']\", '', cleaned_string)\n",
        "    return cleaned_string\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **get_ner:** Main motive of this function is to create NER Labels in ***CoNLL*** format. It takes these Input Parameter i.e. Splited Text, Term, From & To positions of the Term in Text and NER Label which would be changed and sent back by updating the NER Label's for the Term. NER Label's are {0: Others, 1: Begin, 2: Intermediate}. The Output Parameter would be Split text & NER Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "k2lXXZLHdEUM"
      },
      "outputs": [],
      "source": [
        "def get_ner(split_text,term,from_to,ner_label):\n",
        "  # split_text = text.split()\n",
        "  split_term = term.split()\n",
        "\n",
        "  next=0\n",
        "  last=0\n",
        "  word_range=[]\n",
        "  t_word_range=[]\n",
        "  index_range=[]\n",
        "\n",
        "  for k,v in enumerate(split_text):\n",
        "    word_range.append(v)\n",
        "    t_word_range.append( get_clean_text(v))\n",
        "    if v[0] not in [\"'\",'\"']:\n",
        "      index_range.append(next)\n",
        "    else:\n",
        "      index_range.append(next+1)\n",
        "    # print(v[0])\n",
        "    if k == 0:\n",
        "      last = len(v)+1\n",
        "       \n",
        "    else:\n",
        "      last=last+len(v)+1\n",
        "       \n",
        "    next = last\n",
        "  t_index = 0\n",
        "  try:\n",
        "    t_index = index_range.index(from_to)\n",
        "    # print(t_index)\n",
        "  except:\n",
        "     \n",
        "    try:\n",
        "      t_index = t_word_range.index(split_term[0] )\n",
        "    except:\n",
        "      for k,i in enumerate(t_word_range):\n",
        "        if split_term[0] in i:\n",
        "          t_index = k\n",
        "       \n",
        "  end = t_index + len(split_term)\n",
        "  \n",
        "  for k,v in enumerate(range(t_index,end)):\n",
        "    if k==0:\n",
        "      ner_label[v]= 1\n",
        "    else:\n",
        "      ner_label[v]= 2\n",
        "  # print(ner_label)\n",
        "  return word_range, ner_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **get_dataset:** This is the main function which would be called for pre-processing the xml-data. Only Input Parameter is XML_Data and Output Parameter is a list of Dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "lj1JzFqpSkW0"
      },
      "outputs": [],
      "source": [
        "def get_dataset(xml_dict):\n",
        "  dataset_aspect = []\n",
        "  all_lb = set()\n",
        "  for k,v in enumerate(xml_dict['sentences']['sentence']):\n",
        "    ds = {}\n",
        "    # if k == 5:\n",
        "    #   break\n",
        "    dict_keys = v.keys()\n",
        "    ds['ID'] = v['@id']\n",
        "    ds['text'] = v['text']\n",
        "    ds['NER_INFERENCE'] ={}\n",
        "    ds['NER_INFERENCE']['term']=[]\n",
        "    ds['NER_INFERENCE']['polarity']=[]\n",
        "    ds['NER_INFERENCE']['from_to']=[]\n",
        "    ds['NER_INFERENCE']['text']= v['text'].split()\n",
        "    ds['NER_INFERENCE']['ner_label']= [0]*len(v['text'].split())\n",
        "    ds['categories']={}\n",
        "    ds['categories']['category']=[]\n",
        "    ds['categories']['polarity']=[]\n",
        "    # print( v['aspectCategories'])\n",
        "    # print(dict_keys)\n",
        "    if 'aspectTerms' in dict_keys:\n",
        "      if type(v['aspectTerms']['aspectTerm'])==dict:\n",
        "        # tmp_dict={}\n",
        "        ds['NER_INFERENCE']['term'].append(v['aspectTerms']['aspectTerm']['@term'])\n",
        "        ds['NER_INFERENCE']['polarity'].append(v['aspectTerms']['aspectTerm']['@polarity'])\n",
        "        ds['NER_INFERENCE']['from_to'].append((int(v['aspectTerms']['aspectTerm']['@from']),int(v['aspectTerms']['aspectTerm']['@to'])))\n",
        "        ds['NER_INFERENCE']['text'],ds['NER_INFERENCE']['ner_label']= get_ner(\n",
        "                                                                                ds['NER_INFERENCE']['text'],\n",
        "                                                                                v['aspectTerms']['aspectTerm']['@term'],\n",
        "                                                                                int(v['aspectTerms']['aspectTerm']['@from']),\n",
        "                                                                                  ds['NER_INFERENCE']['ner_label']\n",
        "                                                                              )\n",
        "\n",
        "      else:\n",
        "        for key,value in enumerate(v['aspectTerms']['aspectTerm']):\n",
        "          # tmp_dict={}\n",
        "          ds['NER_INFERENCE']['term'].append(value['@term'])\n",
        "          ds['NER_INFERENCE']['polarity'].append(value['@polarity'])\n",
        "          ds['NER_INFERENCE']['from_to'].append((int(value['@from']),int(value['@to'])))\n",
        "          # try:\n",
        "          ds['NER_INFERENCE']['text'],ds['NER_INFERENCE']['ner_label']= get_ner(\n",
        "                                                                                ds['NER_INFERENCE']['text'],\n",
        "                                                                                value['@term'],\n",
        "                                                                                int(value['@from']),\n",
        "                                                                                  ds['NER_INFERENCE']['ner_label']\n",
        "                                                                              )\n",
        "          # except Exception as e:\n",
        "          #   print(e)\n",
        "          #   print('Text: ',ds['text'],' Term: ',value['@term'],\" from: \",int(value['@from']))\n",
        "          #   return \"error\"\n",
        "\n",
        "    if 'aspectCategories' in dict_keys :\n",
        "      # print( v['aspectCategories'])\n",
        "      if type(v['aspectCategories']['aspectCategory'])==dict:\n",
        "        # print( v['aspectCategories']['aspectCategory']) #['@category']\n",
        "        ds['categories']['category'].append(v['aspectCategories']['aspectCategory']['@category'])\n",
        "        ds['categories']['polarity'].append(v['aspectCategories']['aspectCategory']['@polarity'])\n",
        "        all_lb.add(v['aspectCategories']['aspectCategory']['@category'])\n",
        "      else:\n",
        "        # print(v['aspectCategories']['aspectCategory'])\n",
        "        for key,value in enumerate(v['aspectCategories']['aspectCategory']):\n",
        "          # print(value['@category'])\n",
        "          ds['categories']['category'].append(value['@category'])\n",
        "          ds['categories']['polarity'].append(value['@polarity'])\n",
        "          all_lb.add(value['@category'])\n",
        "      # pass\n",
        "    dataset_aspect.append(ds)\n",
        "  return dataset_aspect,all_lb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PN1NCK78hmwZ",
        "outputId": "c660ea93-7be5-4a0c-ce88-61031e63e04b"
      },
      "outputs": [],
      "source": [
        "dataset_apect,all_lb= get_dataset(xml_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### List of Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ambience', 'anecdotes/miscellaneous', 'food', 'price', 'service'}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_lb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### After Pre-processing just vewing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'ID': '3121',\n",
              "  'text': 'But the staff was so horrible to us.',\n",
              "  'NER_INFERENCE': {'term': ['staff'],\n",
              "   'polarity': ['negative'],\n",
              "   'from_to': [(8, 13)],\n",
              "   'text': ['But', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us.'],\n",
              "   'ner_label': [0, 0, 1, 0, 0, 0, 0, 0]},\n",
              "  'categories': {'category': ['service'], 'polarity': ['negative']}}]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_apect[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"artifacts/data/process_data.jsonl\", \"w\") as final:\n",
        "    json.dump(dataset_apect, final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now Loading Dataset in Huggingface Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 3041 examples [00:00, 77302.29 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['ID', 'text', 'NER_INFERENCE', 'categories'],\n",
              "        num_rows: 3041\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"artifacts/data/process_data.jsonl\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Doing Train Test split and saving the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving the dataset (1/1 shards): 100%|██████████| 2736/2736 [00:00<00:00, 67544.15 examples/s]\n",
            "Saving the dataset (1/1 shards): 100%|██████████| 305/305 [00:00<00:00, 50104.29 examples/s]\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset['train'].train_test_split(test_size=0.1,seed=199)\n",
        "\n",
        "dataset.save_to_disk('artifacts/dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text = \"\"\"Don't go alone---even two people isn't 'enough' for the whole experience, with pickles and a selection of meats and seafoods.\"\"\"\n",
        "# from nltk.tokenize import TweetTokenizer\n",
        "# tokenizer = TweetTokenizer()\n",
        "\n",
        "# tokenizer.tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mp31HXcS-kG",
        "outputId": "789f2173-dc06-4f06-b5d5-15a0d4325442"
      },
      "outputs": [],
      "source": [
        "# text = \"Don't go alone---even two people isn't enough for the whole experience, with pickles and a selection of meats and seafoods.\"\n",
        "# term=\"selection of meats and seafoods\"\n",
        "# from_to =(91,122)\n",
        "\n",
        "# split_text = text.split()\n",
        "# ner_label = [0]*len(split_text)\n",
        "# # print(split_text)\n",
        "# def get_ner(split_text,term,from_to,ner_label):\n",
        "#   # split_text = text.split()\n",
        "#   split_term = term.split()\n",
        "\n",
        "#   next=0\n",
        "#   last=0\n",
        "#   word_range=[]\n",
        "#   index_range=[]\n",
        "#   for k,v in enumerate(split_text):\n",
        "#     word_range.append(v)\n",
        "#     index_range.append(next)\n",
        "#     if k == 0:\n",
        "#       last = len(v)+1\n",
        "#     else:\n",
        "#       last=last+len(v)+1\n",
        "#     next = last\n",
        "#   t_index = index_range.index(from_to[0])\n",
        "#   end = t_index + len(split_term)\n",
        "#   for k,v in enumerate(range(t_index,end)):\n",
        "#     if k==0:\n",
        "#       ner_label[v]= 1\n",
        "#     else:\n",
        "#       ner_label[v]= 2\n",
        "#   # print(ner_label)\n",
        "#   return word_range, ner_label\n",
        "#   # print(split_text)\n",
        "# word_range, ner_label=get_ner(split_text,term,from_to,ner_label)\n",
        "\n",
        "# word_range, ner_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMHhAxTwuyl6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
